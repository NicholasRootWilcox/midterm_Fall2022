{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasRootWilcox/midterm_Fall2022/blob/main/WILCOX_Nicholas_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db07fbe",
      "metadata": {
        "id": "4db07fbe",
        "outputId": "442ccd73-591d-47c9-97b2-8def3bedbe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ad50e43338a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuadraticDiscriminantAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionBoundaryDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DecisionBoundaryDisplay' from 'sklearn.inspection' (/usr/local/lib/python3.7/dist-packages/sklearn/inspection/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "## INSTALLING DATA\n",
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "## ashdgfjkawdfnv;lkansdfv[kpjawfklvhjafsk'vn'\n",
        "## askhvajvb]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## for statistical tests\n",
        "import scipy\n",
        "#import statsmodels.formula.api as smf\n",
        "#import statsmodels.api as sm\n",
        "\n",
        "## for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
        "\n",
        "# Import Gaussian Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Import Evaluation Metric\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "## for explainer\n",
        "#from lime import lime_tabular\n",
        "\n",
        "\n",
        "!pip3 install pandas\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8d2a7b",
      "metadata": {
        "id": "9c8d2a7b"
      },
      "outputs": [],
      "source": [
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA \n",
        "# Convert dataset to a pandas dataframe:\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "6Q5RjyjhZJQl"
      },
      "id": "6Q5RjyjhZJQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nicholas\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']"
      ],
      "metadata": {
        "id": "yxFxk3ZRJpEt"
      },
      "id": "yxFxk3ZRJpEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E4y6JheWPaTh"
      },
      "id": "E4y6JheWPaTh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7690a55",
      "metadata": {
        "id": "b7690a55"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TASK: LOAD DATA THAT WAS GIVEN TO YOU\n",
        "# Convert dataset to a pandas dataframe\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "## DATASET type = 'object', 5 indexes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: INSPECT DATA\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "print(f\"DATASET EMPTY: \\t\\t{dataset.empty}\") # Empty\n",
        "print(f\"DATASET SHAPE:  \\t{dataset.shape}\") # Attribute\n",
        "print(f\"DATASET SIZE: \\t\\t{dataset.size}\") # Empty\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET ISNULL VALUE: \\t{dataset.isnull().sum()}\") # Is Null? \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"TOTAL MISSING: \\t{dataset.isnull().sum().sum()}\") # Total Missing\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET HEAD: \\n{dataset.head()}\") # Method\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.dtypes}\") # Types\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.info()}\") # Info \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET COL: \\t{dataset.columns}\") # Col names\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TAIL: \\t{dataset.tail()}\") # TAIL\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "##print(f\"DATASET MIN: \\t{dataset['ColumnName'].max()}\")\n",
        "\n",
        "##print('MIN = \\t\\t\\t\\t\\t', np.min(dataset)) \n",
        "\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rpFklfjpYClF"
      },
      "id": "rpFklfjpYClF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PROBLEM NUMBER 2 --- INCOMPLETE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mt2YCFxtyFLw"
      },
      "id": "mt2YCFxtyFLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram,boxplots, heatmap\n",
        "# Check variable stasttics\n",
        "# Check NaN values, etc.\n",
        "# Visualize correlation\n",
        "\n",
        "# PLOT \n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "newdf = dataset.select_dtypes(include=numerics)\n",
        "#print(newdf)\n",
        "plt.plot(newdf)\n",
        "\n",
        "#newDF is the variable\n",
        "\n"
      ],
      "metadata": {
        "id": "20lC9XhOyUik"
      },
      "id": "20lC9XhOyUik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQUEEZED IMAGE PLOT \n",
        "my_squeezed_image = newdf.squeeze();\n",
        "#plt.imshow(my_squeezed_image)\n",
        "plt.imshow(my_squeezed_image)\n",
        "## why is it full??"
      ],
      "metadata": {
        "id": "Tw8YKXVklSXs"
      },
      "id": "Tw8YKXVklSXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### boxplot -- wont work\n"
      ],
      "metadata": {
        "id": "fYerqUmf_LsC"
      },
      "id": "fYerqUmf_LsC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of NaN pixels in readable columns\n",
        "print('# OF NaN PIXELS = \\t\\t', np.nancumsum(newdf))"
      ],
      "metadata": {
        "id": "Rjl9PnzMleWs"
      },
      "id": "Rjl9PnzMleWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Given coordinates, heatmap and boxplot are not possible. only a simple plot"
      ],
      "metadata": {
        "id": "LJ47pdrsmzW7"
      },
      "id": "LJ47pdrsmzW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK: DATA ENGINEERING, FEATURE ENGINEERING\n",
        "# Use head() function to return the first 5 rows: \n",
        "# print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "# X = dataset.iloc[:, :-1].values\n",
        "# y = dataset.iloc[:, 4].values \n",
        "#X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "# Y = dataset['Coretype']\n",
        "\n",
        "\n",
        "# print(\"Summary Statistics of the X dataframe \\n\", X.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "3fYJFVKpYGO0"
      },
      "id": "3fYJFVKpYGO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "X = newdf.iloc[:, :-1].values\n",
        "y = newdf.iloc[:, 4].values \n",
        "X = newdf[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', \n",
        "         'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', \n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250',  'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2'\n",
        "       ]]\n",
        "#Y = newdf['Coretype']\n",
        "\n",
        "print(\"Summary Statistics of the X dataframe \\n\", X.describe())"
      ],
      "metadata": {
        "id": "sAlEiUf5_0xd"
      },
      "id": "sAlEiUf5_0xd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split dataset into random train and test subsets:\n",
        "print(\"TEST SIZE = 30%\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) \n",
        "\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "print(\"--\")\n",
        "print(\"TEST SIZE = 20%\")\n",
        "print(\"--\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vn9Be3sqYJx5"
      },
      "id": "Vn9Be3sqYJx5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standardizing variable\n",
        "#Instruction: This step is optional\n",
        "# You can use non, one or all of these scalers to find better model, \n",
        "# i.e. models with better accuracy & precision.\n",
        "# For other scaling or feature engineering methods, check this ref:\n",
        "# https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn+preprocessing#module-sklearn.preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K226Jbn_YMm8"
      },
      "id": "K226Jbn_YMm8",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standardize features by removing mean and scaling to unit variance:\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "X_train_dataset = pd.DataFrame(X_train)\n",
        "print(\"\\n Summary Statistics of the X dataframe after Standard Scaling\\n\", X_train_df.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "EK7VYmqaYRau"
      },
      "id": "EK7VYmqaYRau",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MinmaxScaling features by removing mean and scaling to unit variance:\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "print(\"\\n Summary Statistics of the X dataframe after MinMaxScaler Scaling\\n\", X_train_df.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "cMxoBIqOYTG-"
      },
      "id": "cMxoBIqOYTG-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# SELECT MODEL\n",
        "# classifiers = [\n",
        "#    KNeighborsClassifier(n_neighbors=3),\n",
        "#    DecisionTreeClassifier(max_depth=5),\n",
        "#    SVC(kernel=\"linear\", C=0.025),\n",
        "#    SVC(gamma=2, C=1),\n",
        "#    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "#    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "#    MLPClassifier(alpha=1, max_iter=1000),\n",
        "#    AdaBoostClassifier(),\n",
        "#    GaussianNB(),\n",
        "#    QuadraticDiscriminantAnalysis(),\n",
        "#]\n",
        "\n"
      ],
      "metadata": {
        "id": "jtn0RUnxYUeg"
      },
      "id": "jtn0RUnxYUeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK:\n",
        "# Initialize classifier\n",
        "# Choose 3 classifers\n",
        "# Check the paramters of these classifiers using sklearn manuals\n",
        "# Change the paramters manualy or by using for loop to choose the better model (model with better metrics)\n",
        "\n",
        "## 1.7.3. Gaussian Process Classification (GPC)\n",
        "GaussianProcessClassifier(1.0 * RBF(1.0))\n",
        "\n"
      ],
      "metadata": {
        "id": "LZw9f6BtYWfI"
      },
      "id": "LZw9f6BtYWfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use the KNN classifier to fit data:\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)  \n",
        "\n"
      ],
      "metadata": {
        "id": "5uVh07CzYXe0"
      },
      "id": "5uVh07CzYXe0",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gaussian Process Classifier\n",
        "# Why does this say Gaussian Peocess Classifier but use the Decision Tree????\n",
        "classifier = DecisionTreeClassifier(max_depth=5)\n"
      ],
      "metadata": {
        "id": "AYsgr_4TYYkU"
      },
      "id": "AYsgr_4TYYkU",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Support Vector Machine\n",
        "#classifier = SVC(kernel=\"linear\", C=0.025)\n",
        "\n"
      ],
      "metadata": {
        "id": "W6497WmaYa7c"
      },
      "id": "W6497WmaYa7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "#classifier = SVC(gamma=2, C=1)  \n"
      ],
      "metadata": {
        "id": "Sc9TAjUSYchV"
      },
      "id": "Sc9TAjUSYchV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Gaussian Process Classifier\n",
        "print(X.shape, Y.shape)\n",
        "#model = GaussianProcessClassifier()\n",
        "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# summarize result\n",
        "#print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "#classifier = GaussianProcessClassifier(1.0 * RBF(1.0)) \n",
        "\n",
        "#model = GaussianProcessClassifier(kernel=1*RBF(1.0))\n",
        "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "vccqlwSfYdbc"
      },
      "id": "vccqlwSfYdbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# RandomForestClassifier\n",
        "#classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n"
      ],
      "metadata": {
        "id": "7_WfzYkJYeRu"
      },
      "id": "7_WfzYkJYeRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MLPClassifier\n",
        "classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "-OlDsecBYfAK"
      },
      "id": "-OlDsecBYfAK",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# AdaBoostClassifier\n",
        "classifier = AdaBoostClassifier()\n",
        "\n"
      ],
      "metadata": {
        "id": "E7TptZCPYgTI"
      },
      "id": "E7TptZCPYgTI",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# GnB\n",
        "classifier = GaussianNB()  \n",
        "\n"
      ],
      "metadata": {
        "id": "w-l3MFUEYk3w"
      },
      "id": "w-l3MFUEYk3w",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# QuadraticDiscriminantAnalysis\n",
        "classifier = QuadraticDiscriminantAnalysis()\n"
      ],
      "metadata": {
        "id": "Wz428DdaYmAv"
      },
      "id": "Wz428DdaYmAv",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#TRAIN MODEL\n",
        "classifier.fit(X_train, y_train)  # Train the classifier\n"
      ],
      "metadata": {
        "id": "ifoKAhjfYm52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "08908c44-efff-4099-af05-47fa01662c01"
      },
      "id": "ifoKAhjfYm52",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-9d6c70b6223f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TRAIN MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \"\"\"\n\u001b[1;32m    842\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     ]:\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: PREDICT NEW VALUES\n",
        "# Predict y data with classifier: \n",
        "y_predict = classifier.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfxyBu8WYn8b"
      },
      "id": "YfxyBu8WYn8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#TASK: EVALUATE RESULTS\n",
        "# Print results: \n",
        "print(confusion_matrix(y_test, y_predict))\n",
        "print(classification_report(y_test, y_predict)) \n"
      ],
      "metadata": {
        "id": "mfeU-JGQYpdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c918e360-a3ca-4aff-9976-3f065496a1ba"
      },
      "id": "mfeU-JGQYpdV",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-c5511efb8b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TASK: EVALUATE RESULTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Print results:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Evaluate label (subsets) accuracy\n",
        "print(f\"ACCURACY: {accuracy_score(y_test, y_predict)\")\n",
        "\n",
        "print(y_test, y_predict)"
      ],
      "metadata": {
        "id": "3MLlvUvGYrld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c050f035-674b-4436-f298-7a767fceb2d2"
      },
      "id": "3MLlvUvGYrld",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-00311820ea9f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(f\"ACCURACY: {accuracy_score(y_test, y_predict)\")\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d3f636",
      "metadata": {
        "id": "b4d3f636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "c284e639-6557-4179-fb8d-18aad3903f1d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-1d9261196f35>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Tip for modularization\n",
        "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
        "\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}