{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasRootWilcox/midterm_Fall2022/blob/main/WILCOX_Nicholas_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db07fbe",
      "metadata": {
        "id": "4db07fbe"
      },
      "outputs": [],
      "source": [
        "## INSTALLING DATA\n",
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "## ashdgfjkawdfnv;lkansdfv[kpjawfklvhjafsk'vn'\n",
        "## askhvajvb]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## for statistical tests\n",
        "import scipy\n",
        "#import statsmodels.formula.api as smf\n",
        "#import statsmodels.api as sm\n",
        "\n",
        "## for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
        "\n",
        "# Import Gaussian Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Import Evaluation Metric\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "## for explainer\n",
        "#from lime import lime_tabular\n",
        "\n",
        "\n",
        "!pip3 install pandas\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8d2a7b",
      "metadata": {
        "id": "9c8d2a7b"
      },
      "outputs": [],
      "source": [
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA \n",
        "# Convert dataset to a pandas dataframe:\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "6Q5RjyjhZJQl"
      },
      "id": "6Q5RjyjhZJQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nicholas\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']"
      ],
      "metadata": {
        "id": "yxFxk3ZRJpEt"
      },
      "id": "yxFxk3ZRJpEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E4y6JheWPaTh"
      },
      "id": "E4y6JheWPaTh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7690a55",
      "metadata": {
        "id": "b7690a55"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TASK: LOAD DATA THAT WAS GIVEN TO YOU\n",
        "# Convert dataset to a pandas dataframe\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "## DATASET type = 'object', 5 indexes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: INSPECT DATA\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "print(f\"DATASET EMPTY: \\t\\t{dataset.empty}\") # Empty\n",
        "print(f\"DATASET SHAPE:  \\t{dataset.shape}\") # Attribute\n",
        "print(f\"DATASET SIZE: \\t\\t{dataset.size}\") # Empty\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET ISNULL VALUE: \\t{dataset.isnull().sum()}\") # Is Null? \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"TOTAL MISSING: \\t{dataset.isnull().sum().sum()}\") # Total Missing\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET HEAD: \\n{dataset.head()}\") # Method\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.dtypes}\") # Types\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.info()}\") # Info \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET COL: \\t{dataset.columns}\") # Col names\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TAIL: \\t{dataset.tail()}\") # TAIL\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "##print(f\"DATASET MIN: \\t{dataset['ColumnName'].max()}\")\n",
        "\n",
        "##print('MIN = \\t\\t\\t\\t\\t', np.min(dataset)) \n",
        "\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rpFklfjpYClF"
      },
      "id": "rpFklfjpYClF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PROBLEM NUMBER 2 --- INCOMPLETE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mt2YCFxtyFLw"
      },
      "id": "mt2YCFxtyFLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram,boxplots, heatmap\n",
        "# Check variable stasttics\n",
        "# Check NaN values, etc.\n",
        "# Visualize correlation\n",
        "\n",
        "# PLOT \n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "newdf = dataset.select_dtypes(include=numerics)\n",
        "#print(newdf)\n",
        "plt.plot(newdf)\n",
        "\n",
        "#newDF is the variable\n",
        "\n"
      ],
      "metadata": {
        "id": "20lC9XhOyUik"
      },
      "id": "20lC9XhOyUik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQUEEZED IMAGE PLOT \n",
        "my_squeezed_image = newdf.squeeze();\n",
        "#plt.imshow(my_squeezed_image)\n",
        "plt.imshow(my_squeezed_image)\n",
        "## why is it full??"
      ],
      "metadata": {
        "id": "Tw8YKXVklSXs"
      },
      "id": "Tw8YKXVklSXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### boxplot -- wont work\n"
      ],
      "metadata": {
        "id": "fYerqUmf_LsC"
      },
      "id": "fYerqUmf_LsC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of NaN pixels in readable columns\n",
        "print('# OF NaN PIXELS = \\t\\t', np.nancumsum(newdf))"
      ],
      "metadata": {
        "id": "Rjl9PnzMleWs"
      },
      "id": "Rjl9PnzMleWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Given coordinates, heatmap and boxplot are not possible. only a simple plot"
      ],
      "metadata": {
        "id": "LJ47pdrsmzW7"
      },
      "id": "LJ47pdrsmzW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK: DATA ENGINEERING, FEATURE ENGINEERING\n",
        "# Use head() function to return the first 5 rows: \n",
        "# print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "# X = dataset.iloc[:, :-1].values\n",
        "# y = dataset.iloc[:, 4].values \n",
        "#X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "# Y = dataset['Coretype']\n",
        "\n",
        "\n",
        "# print(\"Summary Statistics of the X dataframe \\n\", X.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "3fYJFVKpYGO0"
      },
      "id": "3fYJFVKpYGO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "X = newdf.iloc[:, :-1].values\n",
        "y = newdf.iloc[:, 4].values \n",
        "X = newdf[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', \n",
        "         'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', \n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250',  'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2'\n",
        "       ]]\n",
        "#Y = newdf['Coretype']\n",
        "\n",
        "print(\"Summary Statistics of the X dataframe \\n\", X.describe())"
      ],
      "metadata": {
        "id": "sAlEiUf5_0xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d1696f-be6d-4417-dded-6937c079b02b"
      },
      "id": "sAlEiUf5_0xd",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Seq             Name      RAJ2000      DEJ2000  Signi070    Sp070  e_Sp070  \\\n",
            "0    1  182154.6-025557  18 21 54.63  -02 55 57.2       0.0  0.00825    0.015   \n",
            "1    2  182215.5-030107  18 22 15.58  -03 01 07.9       0.0  0.03570    0.015   \n",
            "2    3  182246.4-032847  18 22 46.43  -03 28 47.8       2.5 -0.03080    0.015   \n",
            "3    4  182254.4-033415  18 22 54.46  -03 34 15.6       0.0  0.01500    0.015   \n",
            "4    5  182303.8-032358  18 23 03.86  -03 23 58.0       0.0  0.00736    0.015   \n",
            "\n",
            "   Sp070/Sbg070  Sconv070  Stot070  ...  FWHMaNH2 FWHMbNH2 PANH2 NSED  \\\n",
            "0          0.20   0.10200 -0.19500  ...        40       22    36    2   \n",
            "1          0.82   0.44100  0.09030  ...        32       22   107    2   \n",
            "2         -0.62  -0.38000 -0.00871  ...        39       23   123    2   \n",
            "3          0.02   0.00569  0.10100  ...        28       26   177    2   \n",
            "4          0.14   0.06140  0.00919  ...        31       21    76    2   \n",
            "\n",
            "   CSARflag      Coretype                            NSIMBAD  \\\n",
            "0         0  starless                                          \n",
            "1         0  starless                                          \n",
            "2         0  starless                                          \n",
            "3         2  starless                                          \n",
            "4         2  prestellar                                        \n",
            "\n",
            "                 NSPITZER            Com  Simbad  \n",
            "0                                         Simbad  \n",
            "1                                         Simbad  \n",
            "2                                         Simbad  \n",
            "3                                         Simbad  \n",
            "4                                         Simbad  \n",
            "\n",
            "[5 rows x 68 columns]\n",
            "Summary Statistics of the X dataframe \n",
            "           Signi070       Sp070     e_Sp070  Sp070/Sbg070    Sconv070  \\\n",
            "count   749.000000  749.000000  749.000000    749.000000  749.000000   \n",
            "mean     45.114419    0.344469    0.078498      1.113178    1.357122   \n",
            "std     318.554582    2.785091    0.369381      9.712839   14.534403   \n",
            "min       0.000000   -5.990000    0.015000     -1.920000  -29.900000   \n",
            "25%       0.000000   -0.016700    0.015000     -0.150000   -0.116000   \n",
            "50%       0.400000    0.006430    0.015000      0.020000    0.020200   \n",
            "75%       4.100000    0.020000    0.017000      0.150000    0.211000   \n",
            "max    6341.000000   51.200000    7.400000    197.250000  305.000000   \n",
            "\n",
            "          Stot070   e_Stot070     Signi160       Sp160     e_Sp160  ...  \\\n",
            "count  749.000000  749.000000   749.000000  749.000000  749.000000  ...   \n",
            "mean     2.240045    0.184793    42.465955    1.847081    0.262733  ...   \n",
            "std     24.151726    0.911049   195.871337    8.594132    0.761529  ...   \n",
            "min    -25.000000    0.015000     0.000000   -0.525000    0.020000  ...   \n",
            "25%     -0.117000    0.020000     0.000000    0.107000    0.045000  ...   \n",
            "50%      0.015400    0.034000     0.000000    0.191000    0.071000  ...   \n",
            "75%      0.213000    0.067000    12.900000    0.481000    0.170000  ...   \n",
            "max    452.000000   19.000000  3358.000000  145.000000    9.400000  ...   \n",
            "\n",
            "        e_Stot500    FWHMa500    FWHMb500       PA500     SigniNH2  \\\n",
            "count  749.000000  749.000000  749.000000  749.000000   749.000000   \n",
            "mean     0.271840   48.887850   38.698264  124.178905    36.133778   \n",
            "std      0.213699   12.482104    6.796095   51.919106    68.693553   \n",
            "min      0.010000   36.000000   36.000000   30.000000     6.000000   \n",
            "25%      0.130000   40.000000   36.000000   85.000000    11.300000   \n",
            "50%      0.200000   46.000000   36.000000  127.000000    17.800000   \n",
            "75%      0.340000   55.000000   38.000000  167.000000    34.600000   \n",
            "max      1.300000  126.000000   85.000000  210.000000  1331.000000   \n",
            "\n",
            "             NpH2    NpH2/Nbg     NconvH2       NbgH2    FWHMaNH2  \n",
            "count  749.000000  749.000000  749.000000  749.000000  749.000000  \n",
            "mean     5.919092    0.482136    2.632710   10.679573   40.248331  \n",
            "std     10.529842    0.418449    4.175503    6.441541   15.523994  \n",
            "min      0.700000    0.090000    0.200000    3.000000   18.000000  \n",
            "25%      2.000000    0.230000    0.900000    7.000000   29.000000  \n",
            "50%      3.000000    0.350000    2.000000    9.000000   36.000000  \n",
            "75%      6.000000    0.580000    3.000000   10.000000   48.000000  \n",
            "max    200.000000    3.910000   70.000000   50.000000  122.000000  \n",
            "\n",
            "[8 rows x 46 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split dataset into random train and test subsets:\n",
        "\n",
        "print(\"TEST SIZE = 20%\")\n",
        "print(\"--\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vn9Be3sqYJx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4fb732-0980-45bb-e02b-fb3ef3448734"
      },
      "id": "Vn9Be3sqYJx5",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST SIZE = 20%\n",
            "--\n",
            "(100, 46) (100,)\n",
            "(80, 46) (80,)\n",
            "(20, 46) (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standardizing variable\n",
        "#Instruction: This step is optional\n",
        "# You can use non, one or all of these scalers to find better model, \n",
        "# i.e. models with better accuracy & precision.\n",
        "# For other scaling or feature engineering methods, check this ref:\n",
        "# https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn+preprocessing#module-sklearn.preprocessing\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K226Jbn_YMm8"
      },
      "id": "K226Jbn_YMm8",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MinmaxScaling features by removing mean and scaling to unit variance:\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "print(\"\\n Summary Statistics of the X dataframe after MinMaxScaler Scaling\\n\", X_train_df.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "cMxoBIqOYTG-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c56abb2-537d-4510-9547-9675d5ff6752"
      },
      "id": "cMxoBIqOYTG-",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Summary Statistics of the X dataframe after MinMaxScaler Scaling\n",
            "               0          1          2          3          4          5   \\\n",
            "count  80.000000  80.000000  80.000000  80.000000  80.000000  80.000000   \n",
            "mean    0.482999   0.614058   0.517764   0.461221   0.477458   0.493357   \n",
            "std     0.221016   0.187866   0.233761   0.188772   0.206371   0.256969   \n",
            "min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
            "25%     0.322412   0.510402   0.378890   0.344177   0.347421   0.309337   \n",
            "50%     0.508558   0.622864   0.526345   0.437556   0.488384   0.501874   \n",
            "75%     0.644164   0.745286   0.651266   0.572239   0.628881   0.674262   \n",
            "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
            "\n",
            "              6          7          8          9   ...         36         37  \\\n",
            "count  80.000000  80.000000  80.000000  80.000000  ...  80.000000  80.000000   \n",
            "mean    0.463027   0.536805   0.499163   0.567313  ...   0.463859   0.549297   \n",
            "std     0.233369   0.230873   0.237517   0.241621  ...   0.214128   0.208005   \n",
            "min     0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000   \n",
            "25%     0.299812   0.378554   0.350178   0.423873  ...   0.350015   0.410765   \n",
            "50%     0.452611   0.545315   0.509022   0.547614  ...   0.459549   0.563158   \n",
            "75%     0.635131   0.698607   0.633811   0.774868  ...   0.585266   0.711560   \n",
            "max     1.000000   1.000000   1.000000   1.000000  ...   1.000000   1.000000   \n",
            "\n",
            "              38         39         40         41         42         43  \\\n",
            "count  80.000000  80.000000  80.000000  80.000000  80.000000  80.000000   \n",
            "mean    0.378784   0.517185   0.456995   0.569127   0.530336   0.569713   \n",
            "std     0.169008   0.226309   0.199083   0.185657   0.189719   0.218881   \n",
            "min     0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
            "25%     0.251894   0.370063   0.340997   0.456052   0.411785   0.422512   \n",
            "50%     0.389520   0.553870   0.461056   0.574172   0.543746   0.592617   \n",
            "75%     0.469235   0.655603   0.598082   0.695546   0.639984   0.726593   \n",
            "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
            "\n",
            "              44         45  \n",
            "count  80.000000  80.000000  \n",
            "mean    0.443452   0.481397  \n",
            "std     0.205116   0.217898  \n",
            "min     0.000000   0.000000  \n",
            "25%     0.298126   0.300942  \n",
            "50%     0.432159   0.492871  \n",
            "75%     0.580194   0.636852  \n",
            "max     1.000000   1.000000  \n",
            "\n",
            "[8 rows x 46 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# SELECT MODEL\n",
        "# classifiers = [\n",
        "#    KNeighborsClassifier(n_neighbors=3),\n",
        "#    DecisionTreeClassifier(max_depth=5),\n",
        "#    SVC(kernel=\"linear\", C=0.025),\n",
        "#    SVC(gamma=2, C=1),\n",
        "#    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "#    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "#    MLPClassifier(alpha=1, max_iter=1000),\n",
        "#    AdaBoostClassifier(),\n",
        "#    GaussianNB(),\n",
        "#    QuadraticDiscriminantAnalysis(),\n",
        "#]\n",
        "\n"
      ],
      "metadata": {
        "id": "jtn0RUnxYUeg"
      },
      "id": "jtn0RUnxYUeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK:\n",
        "# Initialize classifier\n",
        "# Choose 3 classifers\n",
        "# Check the paramters of these classifiers using sklearn manuals\n",
        "# Change the paramters manualy or by using for loop to choose the better model (model with better metrics)\n",
        "\n",
        "## 1.7.3. Gaussian Process Classification (GPC)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LZw9f6BtYWfI"
      },
      "id": "LZw9f6BtYWfI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use the KNN classifier to fit data:\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)  \n",
        "\n"
      ],
      "metadata": {
        "id": "5uVh07CzYXe0"
      },
      "id": "5uVh07CzYXe0",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gaussian Process Classifier\n",
        "# Why does this say Gaussian Peocess Classifier but use the Decision Tree????\n",
        "classifier = DecisionTreeClassifier(max_depth=5)\n"
      ],
      "metadata": {
        "id": "AYsgr_4TYYkU"
      },
      "id": "AYsgr_4TYYkU",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Support Vector Machine\n",
        "#classifier = SVC(kernel=\"linear\", C=0.025)\n",
        "\n"
      ],
      "metadata": {
        "id": "W6497WmaYa7c"
      },
      "id": "W6497WmaYa7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "#classifier = SVC(gamma=2, C=1)  \n"
      ],
      "metadata": {
        "id": "Sc9TAjUSYchV"
      },
      "id": "Sc9TAjUSYchV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Gaussian Process Classifier\n",
        "print(X.shape, y.shape)\n",
        "print(\"asdfj\")\n",
        "model = GaussianProcessClassifier()\n",
        "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# summarize result\n",
        "#print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "#classifier = GaussianProcessClassifier(1.0 * RBF(1.0)) \n",
        "\n",
        "#model = GaussianProcessClassifier(kernel=1*RBF(1.0))\n",
        "classifier = GaussianProcessClassifier(1.0 * RBF(1.0))\n",
        "classifier.fit(X, y)\n",
        "classifier.predict(X)\n",
        "\n",
        "#classifier = GaussianProcessClassifier(1.0 * RBF(1.0)).fit(X, y)\n",
        "#classifier.score(X, y)\n",
        "\n",
        "\n",
        "#X, y = make_classification(n_samples=100, n_features=46, n_informative=15, n_redundant=5, random_state=1)\n",
        "# I have 46 columns and features, but why does the X.shape say only 20??)\n",
        "# summarize the dataset\n",
        "#print(X.shape, y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "vccqlwSfYdbc"
      },
      "id": "vccqlwSfYdbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = 1.0 * RBF(1.0)\n",
        "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0).fit(X_train, y)\n",
        "gpc.score(X_train, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtahyLYTF9p_",
        "outputId": "c790221f-6b92-478f-b018-be90b8595001"
      },
      "id": "WtahyLYTF9p_",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# RandomForestClassifier\n",
        "#classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n"
      ],
      "metadata": {
        "id": "7_WfzYkJYeRu"
      },
      "id": "7_WfzYkJYeRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MLPClassifier\n",
        "classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "-OlDsecBYfAK"
      },
      "id": "-OlDsecBYfAK",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# AdaBoostClassifier\n",
        "classifier = AdaBoostClassifier()\n",
        "\n"
      ],
      "metadata": {
        "id": "E7TptZCPYgTI"
      },
      "id": "E7TptZCPYgTI",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# GnB\n",
        "classifier = GaussianNB()  \n",
        "\n"
      ],
      "metadata": {
        "id": "w-l3MFUEYk3w"
      },
      "id": "w-l3MFUEYk3w",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# QuadraticDiscriminantAnalysis\n",
        "classifier = QuadraticDiscriminantAnalysis()\n"
      ],
      "metadata": {
        "id": "Wz428DdaYmAv"
      },
      "id": "Wz428DdaYmAv",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#TRAIN MODEL\n",
        "classifier.fit(X_train, y_train)  # Train the classifier\n"
      ],
      "metadata": {
        "id": "ifoKAhjfYm52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "08908c44-efff-4099-af05-47fa01662c01"
      },
      "id": "ifoKAhjfYm52",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-9d6c70b6223f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TRAIN MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \"\"\"\n\u001b[1;32m    842\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     ]:\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: PREDICT NEW VALUES\n",
        "# Predict y data with classifier: \n",
        "y_predict = classifier.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfxyBu8WYn8b"
      },
      "id": "YfxyBu8WYn8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#TASK: EVALUATE RESULTS\n",
        "# Print results: \n",
        "print(confusion_matrix(y_test, y_predict))\n",
        "print(classification_report(y_test, y_predict)) \n"
      ],
      "metadata": {
        "id": "mfeU-JGQYpdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "c918e360-a3ca-4aff-9976-3f065496a1ba"
      },
      "id": "mfeU-JGQYpdV",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-c5511efb8b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TASK: EVALUATE RESULTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Print results:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Evaluate label (subsets) accuracy\n",
        "print(f\"ACCURACY: {accuracy_score(y_test, y_predict)\")\n",
        "\n",
        "print(y_test, y_predict)"
      ],
      "metadata": {
        "id": "3MLlvUvGYrld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c050f035-674b-4436-f298-7a767fceb2d2"
      },
      "id": "3MLlvUvGYrld",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-00311820ea9f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(f\"ACCURACY: {accuracy_score(y_test, y_predict)\")\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d3f636",
      "metadata": {
        "id": "b4d3f636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "c284e639-6557-4179-fb8d-18aad3903f1d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-1d9261196f35>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Tip for modularization\n",
        "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
        "\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}