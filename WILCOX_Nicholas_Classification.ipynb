{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NicholasRootWilcox/midterm_Fall2022/blob/main/WILCOX_Nicholas_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db07fbe",
      "metadata": {
        "id": "4db07fbe",
        "outputId": "442ccd73-591d-47c9-97b2-8def3bedbe8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-ad50e43338a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminant_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuadraticDiscriminantAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minspection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionBoundaryDisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DecisionBoundaryDisplay' from 'sklearn.inspection' (/usr/local/lib/python3.7/dist-packages/sklearn/inspection/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "## INSTALLING DATA\n",
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "## ashdgfjkawdfnv;lkansdfv[kpjawfklvhjafsk'vn'\n",
        "## askhvajvb]\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## for statistical tests\n",
        "import scipy\n",
        "#import statsmodels.formula.api as smf\n",
        "#import statsmodels.api as sm\n",
        "\n",
        "## for machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n",
        "\n",
        "# Import Gaussian Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Import Evaluation Metric\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "## for explainer\n",
        "#from lime import lime_tabular\n",
        "\n",
        "\n",
        "!pip3 install pandas\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8d2a7b",
      "metadata": {
        "id": "9c8d2a7b"
      },
      "outputs": [],
      "source": [
        "## for data\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD DATA \n",
        "# Convert dataset to a pandas dataframe:\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "6Q5RjyjhZJQl"
      },
      "id": "6Q5RjyjhZJQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nicholas\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']"
      ],
      "metadata": {
        "id": "yxFxk3ZRJpEt"
      },
      "id": "yxFxk3ZRJpEt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E4y6JheWPaTh"
      },
      "id": "E4y6JheWPaTh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7690a55",
      "metadata": {
        "id": "b7690a55"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TASK: LOAD DATA THAT WAS GIVEN TO YOU\n",
        "# Convert dataset to a pandas dataframe\n",
        "dataset = pd.read_csv('aquila_table1.tsv', delimiter=';',comment='#')\n",
        "X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "y = dataset['Coretype']\n",
        "print(dataset.columns)\n",
        "## DATASET type = 'object', 5 indexes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: INSPECT DATA\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "print(f\"DATASET EMPTY: \\t\\t{dataset.empty}\") # Empty\n",
        "print(f\"DATASET SHAPE:  \\t{dataset.shape}\") # Attribute\n",
        "print(f\"DATASET SIZE: \\t\\t{dataset.size}\") # Empty\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET ISNULL VALUE: \\t{dataset.isnull().sum()}\") # Is Null? \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"TOTAL MISSING: \\t{dataset.isnull().sum().sum()}\") # Total Missing\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET HEAD: \\n{dataset.head()}\") # Method\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.dtypes}\") # Types\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TYPES: \\t{dataset.info()}\") # Info \n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET COL: \\t{dataset.columns}\") # Col names\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(f\"DATASET TAIL: \\t{dataset.tail()}\") # TAIL\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "\n",
        "##print(f\"DATASET MIN: \\t{dataset['ColumnName'].max()}\")\n",
        "\n",
        "##print('MIN = \\t\\t\\t\\t\\t', np.min(dataset)) \n",
        "\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "print(\"....................................\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rpFklfjpYClF"
      },
      "id": "rpFklfjpYClF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PROBLEM NUMBER 2 --- INCOMPLETE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mt2YCFxtyFLw"
      },
      "id": "mt2YCFxtyFLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram,boxplots, heatmap\n",
        "# Check variable stasttics\n",
        "# Check NaN values, etc.\n",
        "# Visualize correlation\n",
        "\n",
        "# PLOT \n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "newdf = dataset.select_dtypes(include=numerics)\n",
        "#print(newdf)\n",
        "plt.plot(newdf)\n",
        "\n",
        "#newDF is the variable\n",
        "\n"
      ],
      "metadata": {
        "id": "20lC9XhOyUik"
      },
      "id": "20lC9XhOyUik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SQUEEZED IMAGE PLOT \n",
        "my_squeezed_image = newdf.squeeze();\n",
        "#plt.imshow(my_squeezed_image)\n",
        "plt.imshow(my_squeezed_image)\n",
        "## why is it full??"
      ],
      "metadata": {
        "id": "Tw8YKXVklSXs"
      },
      "id": "Tw8YKXVklSXs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### boxplot -- wont work\n"
      ],
      "metadata": {
        "id": "fYerqUmf_LsC"
      },
      "id": "fYerqUmf_LsC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of NaN pixels in readable columns\n",
        "print('# OF NaN PIXELS = \\t\\t', np.nancumsum(newdf))"
      ],
      "metadata": {
        "id": "Rjl9PnzMleWs"
      },
      "id": "Rjl9PnzMleWs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Given coordinates, heatmap and boxplot are not possible. only a simple plot"
      ],
      "metadata": {
        "id": "LJ47pdrsmzW7"
      },
      "id": "LJ47pdrsmzW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK: DATA ENGINEERING, FEATURE ENGINEERING\n",
        "# Use head() function to return the first 5 rows: \n",
        "# print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "# X = dataset.iloc[:, :-1].values\n",
        "# y = dataset.iloc[:, 4].values \n",
        "#X = dataset[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', 'FWHMa070',\n",
        "       'FWHMb070', 'PA070', 'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', 'FWHMa160', 'FWHMb160', 'PA160',\n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250', 'FWHMa250', 'FWHMb250', 'PA250', 'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2',\n",
        "       'FWHMbNH2', 'PANH2', 'NSED', 'CSARflag']]\n",
        "# Y = dataset['Coretype']\n",
        "\n",
        "\n",
        "# print(\"Summary Statistics of the X dataframe \\n\", X.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "3fYJFVKpYGO0"
      },
      "id": "3fYJFVKpYGO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head()) \n",
        "# Assign values to the X and y variables:\n",
        "X = newdf.iloc[:, :-1].values\n",
        "y = newdf.iloc[:, 4].values \n",
        "X = newdf[['Signi070', 'Sp070', 'e_Sp070',\n",
        "       'Sp070/Sbg070', 'Sconv070', 'Stot070', 'e_Stot070', \n",
        "         'Signi160', 'Sp160', 'e_Sp160', 'Sp160/Sbg160',\n",
        "       'Sconv160', 'Stot160', 'e_Stot160', \n",
        "       'Signi250', 'Sp250', 'e_Sp250', 'Sp250/Sbg250', 'Sconv250', 'Stot250',\n",
        "       'e_Stot250',  'Signi350', 'Sp350',\n",
        "       'e_Sp350', 'Sp350/Sbg350', 'Sconv350', 'Stot350', 'e_Stot350',\n",
        "       'FWHMa350', 'FWHMb350', 'PA350', 'Signi500', 'Sp500', 'e_Sp500',\n",
        "       'Sp500/Sbg500', 'Stot500', 'e_Stot500', 'FWHMa500', 'FWHMb500', 'PA500',\n",
        "       'SigniNH2', 'NpH2', 'NpH2/Nbg', 'NconvH2', 'NbgH2', 'FWHMaNH2'\n",
        "       ]]\n",
        "Y = newdf['Coretype']\n",
        "\n",
        "# 'FWHMa070', 'FWHMb070', 'PA070', 'FWHMa160', 'FWHMb160', 'PA160', 'FWHMa250', 'FWHMb250', 'PA250'\n",
        "print(\"Summary Statistics of the X dataframe \\n\", X.describe())"
      ],
      "metadata": {
        "id": "sAlEiUf5_0xd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f74a9ba-9068-420a-94bd-774ae199f9d7"
      },
      "id": "sAlEiUf5_0xd",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Seq             Name      RAJ2000      DEJ2000  Signi070    Sp070  e_Sp070  \\\n",
            "0    1  182154.6-025557  18 21 54.63  -02 55 57.2       0.0  0.00825    0.015   \n",
            "1    2  182215.5-030107  18 22 15.58  -03 01 07.9       0.0  0.03570    0.015   \n",
            "2    3  182246.4-032847  18 22 46.43  -03 28 47.8       2.5 -0.03080    0.015   \n",
            "3    4  182254.4-033415  18 22 54.46  -03 34 15.6       0.0  0.01500    0.015   \n",
            "4    5  182303.8-032358  18 23 03.86  -03 23 58.0       0.0  0.00736    0.015   \n",
            "\n",
            "   Sp070/Sbg070  Sconv070  Stot070  ...  FWHMaNH2 FWHMbNH2 PANH2 NSED  \\\n",
            "0          0.20   0.10200 -0.19500  ...        40       22    36    2   \n",
            "1          0.82   0.44100  0.09030  ...        32       22   107    2   \n",
            "2         -0.62  -0.38000 -0.00871  ...        39       23   123    2   \n",
            "3          0.02   0.00569  0.10100  ...        28       26   177    2   \n",
            "4          0.14   0.06140  0.00919  ...        31       21    76    2   \n",
            "\n",
            "   CSARflag      Coretype                            NSIMBAD  \\\n",
            "0         0  starless                                          \n",
            "1         0  starless                                          \n",
            "2         0  starless                                          \n",
            "3         2  starless                                          \n",
            "4         2  prestellar                                        \n",
            "\n",
            "                 NSPITZER            Com  Simbad  \n",
            "0                                         Simbad  \n",
            "1                                         Simbad  \n",
            "2                                         Simbad  \n",
            "3                                         Simbad  \n",
            "4                                         Simbad  \n",
            "\n",
            "[5 rows x 68 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Coretype'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-5d1cf487c8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m        \u001b[0;34m'SigniNH2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NpH2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NpH2/Nbg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NconvH2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NbgH2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FWHMaNH2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m        ]]\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Coretype'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 'FWHMa070', 'FWHMb070', 'PA070', 'FWHMa160', 'FWHMb160', 'PA160', 'FWHMa250', 'FWHMb250', 'PA250'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Coretype'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Split dataset into random train and test subsets:\n",
        "print(\"TEST SIZE = 30%\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30) \n",
        "\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n",
        "print(\"--\")\n",
        "print(\"TEST SIZE = 20%\")\n",
        "print(\"--\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
        "\n",
        "print(X.shape, y.shape)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "Vn9Be3sqYJx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47635464-adee-46e0-88b3-e492b232f506"
      },
      "id": "Vn9Be3sqYJx5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST SIZE = 30%\n",
            "(100, 20) (100,)\n",
            "(70, 20) (70,)\n",
            "(30, 20) (30,)\n",
            "--\n",
            "TEST SIZE = 20%\n",
            "--\n",
            "(100, 20) (100,)\n",
            "(80, 20) (80,)\n",
            "(20, 20) (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standardizing variable\n",
        "#Instruction: This step is optional\n",
        "# You can use non, one or all of these scalers to find better model, \n",
        "# i.e. models with better accuracy & precision.\n",
        "# For other scaling or feature engineering methods, check this ref:\n",
        "# https://scikit-learn.org/stable/modules/classes.html?highlight=sklearn+preprocessing#module-sklearn.preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K226Jbn_YMm8"
      },
      "id": "K226Jbn_YMm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Standardize features by removing mean and scaling to unit variance:\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "X_train_dataset = pd.DataFrame(X_train)\n",
        "print(\"\\n Summary Statistics of the X dataframe after Standard Scaling\\n\", X_train_df.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "EK7VYmqaYRau",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "3260d178-0dfb-4790-bcdb-505957c8d117"
      },
      "id": "EK7VYmqaYRau",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-815d4af051e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n Summary Statistics of the X dataframe after Standard Scaling\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MinmaxScaling features by removing mean and scaling to unit variance:\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "X_train_df = pd.DataFrame(X_train)\n",
        "print(\"\\n Summary Statistics of the X dataframe after MinMaxScaler Scaling\\n\", X_train_df.describe())\n",
        "\n"
      ],
      "metadata": {
        "id": "cMxoBIqOYTG-"
      },
      "id": "cMxoBIqOYTG-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# SELECT MODEL\n",
        "# classifiers = [\n",
        "#    KNeighborsClassifier(n_neighbors=3),\n",
        "#    DecisionTreeClassifier(max_depth=5),\n",
        "#    SVC(kernel=\"linear\", C=0.025),\n",
        "#    SVC(gamma=2, C=1),\n",
        "#    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "#    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "#    MLPClassifier(alpha=1, max_iter=1000),\n",
        "#    AdaBoostClassifier(),\n",
        "#    GaussianNB(),\n",
        "#    QuadraticDiscriminantAnalysis(),\n",
        "#]\n",
        "\n"
      ],
      "metadata": {
        "id": "jtn0RUnxYUeg"
      },
      "id": "jtn0RUnxYUeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# TASK:\n",
        "# Initialize classifier\n",
        "# Choose 3 classifers\n",
        "# Check the paramters of these classifiers using sklearn manuals\n",
        "# Change the paramters manualy or by using for loop to choose the better model (model with better metrics)\n",
        "\n",
        "## 1.7.3. Gaussian Process Classification (GPC)\n",
        "GaussianProcessClassifier(1.0 * RBF(1.0))\n",
        "\n"
      ],
      "metadata": {
        "id": "LZw9f6BtYWfI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f333a8-ae71-48d2-ef22-6e13a90956c8"
      },
      "id": "LZw9f6BtYWfI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianProcessClassifier(kernel=1**2 * RBF(length_scale=1))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Use the KNN classifier to fit data:\n",
        "classifier = KNeighborsClassifier(n_neighbors=3)  \n",
        "\n"
      ],
      "metadata": {
        "id": "5uVh07CzYXe0"
      },
      "id": "5uVh07CzYXe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Gaussian Process Classifier\n",
        "classifier = DecisionTreeClassifier(max_depth=5)\n"
      ],
      "metadata": {
        "id": "AYsgr_4TYYkU"
      },
      "id": "AYsgr_4TYYkU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Support Vector Machine\n",
        "classifier = SVC(kernel=\"linear\", C=0.025)\n",
        "\n"
      ],
      "metadata": {
        "id": "W6497WmaYa7c"
      },
      "id": "W6497WmaYa7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Support Vector Machine\n",
        "classifier = SVC(gamma=2, C=1)  \n"
      ],
      "metadata": {
        "id": "Sc9TAjUSYchV"
      },
      "id": "Sc9TAjUSYchV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Gaussian Process Classifier\n",
        "#print(X.shape, Y.shape)\n",
        "#model = GaussianProcessClassifier()\n",
        "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "# evaluate model\n",
        "#scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "# summarize result\n",
        "#print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "#classifier = GaussianProcessClassifier(1.0 * RBF(1.0)) \n",
        "\n",
        "#model = GaussianProcessClassifier(kernel=1*RBF(1.0))\n",
        "X, y = make_classification(n_samples=100, n_features=20, n_informative=15, n_redundant=5, random_state=1)\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "vccqlwSfYdbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "053aced2-9ad9-44aa-bccd-b684ed9d3a04"
      },
      "id": "vccqlwSfYdbc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 20) (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# RandomForestClassifier\n",
        "classifier = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n"
      ],
      "metadata": {
        "id": "7_WfzYkJYeRu"
      },
      "id": "7_WfzYkJYeRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# MLPClassifier\n",
        "classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
        "\n"
      ],
      "metadata": {
        "id": "-OlDsecBYfAK"
      },
      "id": "-OlDsecBYfAK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# AdaBoostClassifier\n",
        "classifier = AdaBoostClassifier()\n",
        "\n"
      ],
      "metadata": {
        "id": "E7TptZCPYgTI"
      },
      "id": "E7TptZCPYgTI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# GnB\n",
        "classifier = GaussianNB()  \n",
        "\n"
      ],
      "metadata": {
        "id": "w-l3MFUEYk3w"
      },
      "id": "w-l3MFUEYk3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# QuadraticDiscriminantAnalysis\n",
        "classifier = QuadraticDiscriminantAnalysis()\n"
      ],
      "metadata": {
        "id": "Wz428DdaYmAv"
      },
      "id": "Wz428DdaYmAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#TRAIN MODEL\n",
        "classifier.fit(X_train, y_train)  # Train the classifier\n"
      ],
      "metadata": {
        "id": "ifoKAhjfYm52"
      },
      "id": "ifoKAhjfYm52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TASK: PREDICT NEW VALUES\n",
        "# Predict y data with classifier: \n",
        "y_predict = classifier.predict(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "YfxyBu8WYn8b"
      },
      "id": "YfxyBu8WYn8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#TASK: EVALUATE RESULTS\n",
        "# Print results: \n",
        "print(confusion_matrix(y_test, y_predict))\n",
        "print(classification_report(y_test, y_predict)) \n"
      ],
      "metadata": {
        "id": "mfeU-JGQYpdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9fbe5f83-31c0-450f-9015-8b301a9180ef"
      },
      "id": "mfeU-JGQYpdV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c5511efb8b86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TASK: EVALUATE RESULTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Print results:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Evaluate label (subsets) accuracy\n",
        "print(accuracy_score(y_test, y_predict))\n",
        "\n",
        "print(y_test, y_predict)"
      ],
      "metadata": {
        "id": "3MLlvUvGYrld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a0361dd6-c700-464e-f403-3cac9291114f"
      },
      "id": "3MLlvUvGYrld",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-9cf663cbbb5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate label (subsets) accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d3f636",
      "metadata": {
        "id": "b4d3f636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "c284e639-6557-4179-fb8d-18aad3903f1d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-45-1d9261196f35>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Tip for modularization\n",
        "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py\n",
        "\n",
        "\n",
        "\n",
        "#https://towardsdatascience.com/machine-learning-with-python-classification-complete-tutorial-d2c99dc524ec"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}